{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# STOP WORD REMOVAL"
      ],
      "metadata": {
        "id": "DPkNeJt3pTgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Natural Language Toolkit\n"
      ],
      "metadata": {
        "id": "X4nG7I4fPRpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9EHp5H-ODvb",
        "outputId": "d9f390eb-7c68-48b4-c184-234078e3b09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Total number of stop words"
      ],
      "metadata": {
        "id": "GcojTmgwSTJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Thdsb7bnQwea",
        "outputId": "b873cce1-066d-436f-ecd4-c83fcd69bfd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Inserting text and calculating difference in lengths"
      ],
      "metadata": {
        "id": "do8prpXFooh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Beauty not only is what’s on the outside but is what is on the inside as well. Only thing that should matter about a person is their personality.\"\n",
        "\n",
        "words = [word for word in text.split() if word.lower() not in stop_words]\n",
        "new_text = \" \".join(words)\n",
        "\n",
        "print(new_text)\n",
        "print(\"Old length: \", len(text))\n",
        "print(\"New length: \", len(new_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ7hpt7mOOLd",
        "outputId": "b2f7dab2-f76f-4556-cc32-e8a0ad24911e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beauty what’s outside inside well. thing matter person personality.\n",
            "Old length:  145\n",
            "New length:  67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEMMING"
      ],
      "metadata": {
        "id": "mthzhUJ1qe8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "a = \"smiling smile studying studies studied take taking taker\"\n",
        "words = word_tokenize(a)\n",
        "ps = PorterStemmer()\n",
        "\n",
        "for x in words:\n",
        "\tstem=ps.stem(x)\n",
        "\tprint(\"Stem word for \"+x+\" is \"+stem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRpwIWj9SPnK",
        "outputId": "5ef1b7b0-b76e-4a94-b257-18e7dd5dcea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stem word for smiling is smile\n",
            "Stem word for smile is smile\n",
            "Stem word for studying is studi\n",
            "Stem word for studies is studi\n",
            "Stem word for studied is studi\n",
            "Stem word for take is take\n",
            "Stem word for taking is take\n",
            "Stem word for taker is taker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As we can see, stemming doesn't always provide the most reliable answer as is in the case of studying"
      ],
      "metadata": {
        "id": "8dpOXSxHtRL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LEMMATIZATION"
      ],
      "metadata": {
        "id": "5zIX8Xq2uE3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import\tWordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "wl = WordNetLemmatizer()\n",
        "a = \"smiling smile studying studies studied take taking taker\"\n",
        "tokenization = nltk.word_tokenize(a)\n",
        "for x in tokenization:\n",
        "  w = wl.lemmatize(x)\n",
        "  print(\"Lemma for \"+x+\" is \"+w)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAYiJkCgs6K8",
        "outputId": "97fcedfd-87d8-4cd9-f19b-c8228a4772ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma for smiling is smiling\n",
            "Lemma for smile is smile\n",
            "Lemma for studying is studying\n",
            "Lemma for studies is study\n",
            "Lemma for studied is studied\n",
            "Lemma for take is take\n",
            "Lemma for taking is taking\n",
            "Lemma for taker is taker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOKENIZATION"
      ],
      "metadata": {
        "id": "mhXbIJpN7O7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### First method used for tokenization is split()"
      ],
      "metadata": {
        "id": "u9Bj-j0Y8OTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Mango is a fruit.\"\n",
        "\n",
        "list1=sentence.split()"
      ],
      "metadata": {
        "id": "NlqE4oZi7Nkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9McilUjM4bc",
        "outputId": "b29edfa8-631c-43bc-892d-d633dc0415ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mango', 'is', 'a', 'fruit.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second is using re library"
      ],
      "metadata": {
        "id": "fymC8lRQNTD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "bIAnVDYxM6nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = '\\w+'\n",
        "tokens = re.findall(pattern,sentence,flags =0)\n",
        "\n",
        "# Here '\\w+' represent all the alphanumeric values"
      ],
      "metadata": {
        "id": "CYPhf8fzNYdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyWw48f5NdHd",
        "outputId": "274e2638-9939-4748-8e4f-42968ed13094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mango', 'is', 'a', 'fruit']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Third method is using nltk library"
      ],
      "metadata": {
        "id": "rqGDgYP7O85U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "V_7t_afiNehO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now if we have to split sentences or paragraphs into tokens(words)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "word_tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srBxVp2sPM-O",
        "outputId": "774d73c6-1008-4b92-d866-7af9bada242f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mango', 'is', 'a', 'fruit', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If in case we have to splitr a paragraph into tokens which are in the form of sentences:\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sent_tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiBIJxEgPiNN",
        "outputId": "d236da44-a686-4db6-8476-191b30f92c41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mango is a fruit.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_2 = \"Data Science is on a steep increasing slope. Taking it up as a career is a smart choice\"\n",
        "\n",
        "sent_tokenize(sentence_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua0udJrxP4Hp",
        "outputId": "b00eeb99-d05c-452a-97b0-15a5f770bba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data Science is on a steep increasing slope.',\n",
              " 'Taking it up as a career is a smart choice']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}